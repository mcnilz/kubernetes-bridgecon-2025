---
# You can also start simply with 'default'
theme: bricks
# random image from a curated Unsplash collection by Anthony
# like them? see https://unsplash.com/collections/94734566/slidev
background: https://cover.sli.dev
# some information about your slides (markdown enabled)
title: Welcome to Slidev
info: |
  ## Kubernetes f√ºr bridgeCon 2025
# apply unocss classes to the current slide
class: text-center
# https://sli.dev/features/drawing
drawings:
  persist: false
# slide transition: https://sli.dev/guide/animations.html#slide-transitions
transition: slide-left
# enable MDC Syntax: https://sli.dev/features/mdc
mdc: true
# open graph
# seoMeta:
#  ogImage: https://cover.sli.dev
---

# Kubernetes ‚Äì Praxis f√ºr Entwickler

<!--
Willkommen! Heute geht es um Kubernetes, speziell aus der Sicht von Entwicklern.  
Ihr kennt alle Docker, deshalb steigen wir gleich in die Praxis und Use-Cases f√ºr euch ein.
-->

---

## Agenda

- Warum Kubernetes?
- Die wichtigsten Grundlagen
- Helm & Flux im Schnelldurchlauf
- Live-Demo: GitOps mit Flux **ODER** Monitoring mit Prometheus & Grafana
- Fragen & Diskussion

<!--
Kurzer √úberblick √ºber die Themen heute.  
Demo-Schwerpunkt w√§hlen wir je nach Interesse ‚Äì ich zeige gleich beide Optionen.
-->

---

# Warum Kubernetes?

- ü§ñ **Automatisierung & Selbstheilung**
- üöÄ **Skalierung auf Knopfdruck**
- üîÑ **Zero-Downtime Deployments**
- üåç **Portabilit√§t (Cloud, On-Prem, Hybrid)**
- üìù **Deklarative Konfiguration (YAML)**
- üõ°Ô∏è **Rollen & Rechte (RBAC)**
- üí° **Gro√ües √ñkosystem & Community**

<!--

- Automatisierung: Kubernetes erkennt, wenn Container oder Pods ausfallen und startet sie neu.  
- Skalierung: Ihr k√∂nnt Lastspitzen einfach abfangen, Kubernetes erstellt oder entfernt automatisch Instanzen.
- Zero-Downtime: Updates und Deployments laufen ohne Ausf√§lle, Rollbacks sind einfach m√∂glich.
- Portabilit√§t: Gleicher Stack auf Azure, AWS, GCP, on-prem oder lokal ‚Äì keine Cloud-Bindung!
- Deklarativ: Infrastruktur als Code, alles nachvollziehbar und versionierbar.
- RBAC: Feingranulare Rechte, mehrere Teams k√∂nnen sicher im selben Cluster arbeiten.
- √ñkosystem: Riesige Community, viele Tools und Tutorials ‚Äì keine Insell√∂sung!
-->

---

# Deklarative Konfiguration & Anwendung mit kubectl

- Ressourcen als YAML definieren (‚ÄûInfrastructure as Code‚Äú)
- √Ñnderungen versionierbar & nachvollziehbar
- Anwendung per Befehl:  
  `kubectl apply -f <datei.yaml>`
- Cluster-Zustand wird automatisch angepasst

<!--
Bei Kubernetes beschreiben wir den Soll-Zustand unserer Infrastruktur in YAML-Dateien.  
Diese Definitionen k√∂nnen wir versionieren, gemeinsam pflegen und jederzeit nachvollziehen.
Mit `kubectl apply` werden diese Beschreibungen an das Cluster √ºbertragen ‚Äì Kubernetes sorgt daf√ºr, dass der reale Zustand zum Soll-Zustand passt.
-->

---

# Die wichtigsten Ressourcen in Kubernetes

- **Pod:**  
  Kleinste deploybare Einheit, l√§uft ein oder mehrere Container
- **Deployment:**  
  Verwaltung und Rollout von stateless Anwendungen
- **StatefulSet:**  
  Verwaltung zustandsbehafteter Anwendungen, z.B. Datenbanken
- **Service:**  
  Stellt Netzwerkzugriff auf Pods bereit, sorgt f√ºr Load Balancing

<!-- note:
Die vier Ressourcen sind die Basis f√ºr fast alle Anwendungen in Kubernetes.
Pods enthalten die Container.
Deployments managen stateless Apps, k√ºmmern sich um Updates/Rollbacks.
StatefulSets sind f√ºr alles zust√§ndig, was stabilen Speicher und feste Namen braucht (z.B. Datenbanken).
Services verbinden die Pods miteinander und mit der Au√üenwelt.
-->

---

# Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: whoami-pod
spec:
  containers:
    - name: whoami
      image: docker.io/traefik/whoami
      ports:
        - containerPort: 80
```

- Enth√§lt Container mit gemeinsamem Netzwerk und Storage
- Kurzlebig, wird meist √ºber Deployment gemanaged

<!-- 
Pods laufen direkt im Cluster, meistens mit einem Container (manchmal mehreren, wenn diese eng zusammengeh√∂ren). Sie teilen sich Netzwerk und k√∂nnen gemeinsam auf Volumes zugreifen. Pods sind ‚Äûkurzlebig‚Äú ‚Äì wenn sie sterben, werden sie meist von Deployments neu erzeugt.
-->

---

# Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
        - name: app
          image: docker.io/traefik/whoami
          ports:
            - name: http
              containerPort: 80
```

- Verwalten stateless Anwendungen (Webserver, API, Worker)
- Sorgt f√ºr Skalierung, Updates und Selbstheilung

<!-- 
Deployments sind der Standard f√ºr stateless Anwendungen. Sie beschreiben, wie viele Pods laufen sollen (‚Äûreplicas‚Äú), welches Image verwendet wird, und k√ºmmern sich um Updates und Skalierung. Bei Fehlern sorgt das Deployment daf√ºr, dass die gew√ºnschte Anzahl Pods immer verf√ºgbar ist. 
-->

---

# StatefulSet

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database
spec:
  serviceName: "db"
  replicas: 1
  selector:
    matchLabels:
      app: db
  template:
    metadata:
      labels:
        app: db
    spec:
      containers:
        - name: db
          image: postgres
          ports:
            - containerPort: 5432
          volumeMounts:
            - name: postgres
              mountPath: /data
          env:
          - name: POSTGRES_PASSWORD
            value: admin
  volumeClaimTemplates:
  - metadata:
      name: postgres
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 2Gi
```

- F√ºr zustandsbehaftete Anwendungen (z.B. Datenbanken)
- Erh√§lt feste Namen & stabilen Speicher f√ºr jeden Pod

<!-- 
StatefulSets sind f√ºr Anwendungen gedacht, die stabilen Speicher und eine feste Identit√§t pro Instanz brauchen ‚Äì z.B. Datenbanken oder Message Queues. Jeder Pod bekommt einen stabilen Namen und ein eigenes Volume. Updates und Rollouts erfolgen hier geordnet (nacheinander).
-->

---

# Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  selector:
    app: web
  ports:
    - protocol: TCP
      port: 80
      targetPort: http
  type: LoadBalancer
```

- Macht Pods im Cluster auffindbar und erreichbar
- Verschiedene Typen: ClusterIP, NodePort, LoadBalancer

<!-- 
Services sorgen daf√ºr, dass Pods im Cluster oder von au√üen erreichbar sind. ClusterIP ist nur intern sichtbar, NodePort √∂ffnet einen Port auf jedem Node, LoadBalancer nutzt Cloud-Loadbalancer. Der Service findet automatisch alle passenden Pods √ºber das Label-Selector-Prinzip.
-->

---

# Unterst√ºtzende Ressourcen: ConfigMap

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-config
data:
  APP_ENV: production
  LOG_LEVEL: debug
```

- Speichert Konfiguration als Key/Value-Paare
- Trennung von Code und Konfiguration
- Kann als Umgebungsvariable oder Datei im Pod verwendet werden

<!-- 
Mit ConfigMaps werden Umgebungsvariablen, Konfigurationsdateien usw. verwaltet. Der Code bleibt unver√§ndert, die Konfiguration kann unabh√§ngig davon angepasst werden. Im Pod k√∂nnen ConfigMaps als Umgebungsvariable oder als Datei gemountet werden.
-->

---

# Unterst√ºtzende Ressourcen: Secret

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: example-secret
type: Opaque
data:
  DB_PASSWORD: cGFzc3dvcmQ=    # Base64-encoded!
```

- F√ºr vertrauliche Daten (z.B. Passw√∂rter, Tokens, Zertifikate)
- Werte sind Base64-codiert gespeichert
- K√∂nnen als Umgebungsvariable oder Datei bereitgestellt werden

<!-- 
Secrets werden √§hnlich wie ConfigMaps benutzt, aber f√ºr sensible Daten wie Passw√∂rter und API-Tokens. Die Werte sind Base64-codiert (nicht verschl√ºsselt, aber nicht im Klartext). Secrets k√∂nnen Pods als Umgebungsvariable oder als Datei zur Verf√ºgung gestellt werden.
-->

---

# Unterst√ºtzende Ressourcen: Volume & PersistentVolumeClaim

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: example-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-volume
spec:
  containers:
    - name: app
      image: docker.io/traefik/whoami
      volumeMounts:
        - mountPath: "/data"
          name: my-storage
  volumes:
    - name: my-storage
      persistentVolumeClaim:
        claimName: example-pvc
```

- Volumes: Speicher f√ºr Pods (z.B. f√ºr Logs oder Uploads)
- PersistentVolumeClaim (PVC): Fordert persistenten Speicher an
- Speicher bleibt √ºber Pod-Neustarts erhalten

<!-- 
Volumes sind notwendig, wenn Anwendungen im Container Daten dauerhaft speichern sollen. Mit einem PersistentVolumeClaim fordere ich Speicher vom Cluster an, z.B. f√ºr eine Datenbank oder Datei-Uploads. Im Pod wird das Volume eingebunden, sodass Container darauf zugreifen k√∂nnen ‚Äì auch nach einem Pod-Restart bleibt der Speicher erhalten. 
-->

---

# Ende

---

# f√ºr Besserwisser Nachfragen

---

# Wie funktioniert das Anwenden von Ressourcen in Kubernetes?

- Ressourcen werden als YAML (oder JSON) an die Kubernetes API gesendet (z.B. via `kubectl apply`)
- Die Kubernetes API speichert alle Ressourcen im **etcd**-Cluster (Zustandsdatenbank)
- **Controller** √ºberwachen kontinuierlich den ‚ÄûSoll-Zustand‚Äú (aus YAML) und den ‚ÄûIst-Zustand‚Äú (laufende Pods etc.)

<!--
Kubernetes ist komplett API-gesteuert.  
Alles ‚Äì Pods, Deployments, Services usw. ‚Äì werden als Ressourcen an die API √ºbermittelt.  
Die API speichert den gew√ºnschten Zustand (Soll-Zustand) zentral ab.  
Controller (z.B. Deployment Controller, ReplicaSet Controller) beobachten st√§ndig, ob der Ist-Zustand im Cluster mit dem Soll-Zustand √ºbereinstimmt.  
Wenn etwas abweicht (z.B. ein Pod ist abgest√ºrzt), steuern die Controller gegen (z.B. neuer Pod wird gestartet).
-->

---

# Kubernetes Architektur

```mermaid
flowchart LR
    User([kubectl / User])
    API[API Server]
    etcd[(etcd)]
    Scheduler[Scheduler]
    Controller[Controller]
    Node1[Kubelet auf Node 1]
    Node2[Kubelet auf Node 2]

    User --"apply YAML"--> API
    API --"speichert Zustand"--> etcd
    API --"informiert"--> Controller
    Controller --"verwaltet Ressourcen"--> API
    API --"stellt bereit"--> Scheduler
    Scheduler --"entscheidet Zuweisung"--> API
    Node1 --"fragt Pod-Status ab"--> API
    Node2 --"fragt Pod-Status ab"--> API

```

- API-Server: Drehscheibe f√ºr alle Komponenten
- Scheduler: Entscheidet, welcher Pod auf welchem Node l√§uft
- Kubelet: Fragt den API Server ab und startet die Container

<!-- 
Wichtig: Der API Server kommuniziert nicht direkt mit den Nodes!
Die Kubelets auf den Nodes ‚Äûpullen‚Äú regelm√§√üig den API Server, um neue Aufgaben (Pods) zu bekommen.
Der Scheduler ist derjenige, der Pods bestimmten Nodes zuweist.
Controller sorgen daf√ºr, dass alles im Soll-Zustand bleibt (z.B. ReplicaSet Controller sorgt f√ºr richtige Anzahl Pods).
-->